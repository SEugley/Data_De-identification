{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4bb0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title: Data Privacy Certification Report\n",
    "### Author: Scott Eugley\n",
    "### Date: 10/20/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876f6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data and libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw_data = pd.read_excel(\"/Users/seugley/Desktop/GitHub/Data_De-identification/Customer_Survey_Data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee8d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow down raw data by selecting the columns of interest: CustomerID, Gender, Age, HouseholdIncome, and CardSpendMonthly\n",
    "\n",
    "columns_of_interest = ['CustomerID', 'Gender', 'Age', 'HouseholdIncome', 'CardSpendMonthly']\n",
    "data_of_interest = raw_data[columns_of_interest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a046c450",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Age: 18\n",
      "Maximum Age: 79\n",
      "\n",
      "Top 10 Minimum Household Incomes:\n",
      "135    9\n",
      "311    9\n",
      "321    9\n",
      "338    9\n",
      "427    9\n",
      "510    9\n",
      "600    9\n",
      "604    9\n",
      "655    9\n",
      "666    9\n",
      "Name: HouseholdIncome, dtype: int64\n",
      "\n",
      "Top 10 Maximum Household Incomes:\n",
      "1102    1073\n",
      "2192     995\n",
      "3068     780\n",
      "3212     642\n",
      "4949     575\n",
      "3623     526\n",
      "754      515\n",
      "2061     472\n",
      "4916     437\n",
      "17       424\n",
      "Name: HouseholdIncome, dtype: int64\n",
      "\n",
      "Top 10 Minimum Card Spending Monthly:\n",
      "1657    0.00\n",
      "1716    0.00\n",
      "2799    0.00\n",
      "2878    0.00\n",
      "4099    0.00\n",
      "4714    0.00\n",
      "4890    0.00\n",
      "4025    6.97\n",
      "3304    7.34\n",
      "3549    7.53\n",
      "Name: CardSpendMonthly, dtype: float64\n",
      "\n",
      "Top 10 Maximum Card Spending Monthly:\n",
      "3386    3926.41\n",
      "1523    3104.63\n",
      "1102    2969.39\n",
      "2598    2503.25\n",
      "1298    2461.03\n",
      "508     1978.12\n",
      "206     1899.93\n",
      "2966    1894.91\n",
      "2430    1799.19\n",
      "711     1753.69\n",
      "Name: CardSpendMonthly, dtype: float64\n",
      "Missing values in each column:\n",
      "Gender              0\n",
      "Age                 0\n",
      "HouseholdIncome     0\n",
      "CardSpendMonthly    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === Data Quality Control ===\n",
    "\n",
    "# First start by looking at the min and max values for each of the variables to see if they make sense\n",
    "\n",
    "# min and max values for age\n",
    "min_age = data_of_interest['Age'].min()\n",
    "max_age = data_of_interest['Age'].max()\n",
    "\n",
    "# 10 min and 10 max values for household income\n",
    "top_10_min_income = data_of_interest['HouseholdIncome'].nsmallest(10)\n",
    "top_10_max_income = data_of_interest['HouseholdIncome'].nlargest(10)\n",
    "\n",
    "# 10 min and 10 max values for 'card spend monthly\n",
    "top_10_min_card_spend = data_of_interest['CardSpendMonthly'].nsmallest(10)\n",
    "top_10_max_card_spend = data_of_interest['CardSpendMonthly'].nlargest(10)\n",
    "\n",
    "# Print results\n",
    "print(\"Minimum Age:\", min_age)\n",
    "print(\"Maximum Age:\", max_age)\n",
    "\n",
    "print(\"\\nTop 10 Minimum Household Incomes:\")\n",
    "print(top_10_min_income)\n",
    "print(\"\\nTop 10 Maximum Household Incomes:\")\n",
    "print(top_10_max_income)\n",
    "\n",
    "print(\"\\nTop 10 Minimum Card Spending Monthly:\")\n",
    "print(top_10_min_card_spend)\n",
    "print(\"\\nTop 10 Maximum Card Spending Monthly:\")\n",
    "print(top_10_max_card_spend)\n",
    "\n",
    "# There are some extreme high and low values, however, nothing appears to be out of the question. Some examples: a negative household income, an unusually high max age, a negative card spend monthly\n",
    "\n",
    "# Identify any null, NA, or missing data points\n",
    "\n",
    "columns_to_check = ['Gender', 'Age', 'HouseholdIncome', 'CardSpendMonthly']\n",
    "missing_values = data_of_interest[columns_to_check].isnull().sum()\n",
    "\n",
    "# Print results\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# There are no missing values in the columns of interest\n",
    "\n",
    "# Remove CustomerID column as this has no utility passed this point\n",
    "\n",
    "columns_of_interest = ['Gender', 'Age', 'HouseholdIncome', 'CardSpendMonthly']\n",
    "data_of_interest = raw_data[columns_of_interest]\n",
    "\n",
    "# Change 0 and 1 to male and female respectively as establishing gender makes this data much more useful to the buyer. Made a copy of dataset to avoid warning message\n",
    "data_of_interest = data_of_interest.copy()\n",
    "data_of_interest.loc[:, 'Gender'] = data_of_interest['Gender'].map({0: 'Male', 1: 'Female'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ae403d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Gender AgeGroup IncomeGroup CardSpendGroup  count\n",
      "0   Female    18-35        0-35          0-250    309\n",
      "1   Female    18-35        0-35        251-600    220\n",
      "2   Female    18-35        0-35           601+     25\n",
      "3   Female    18-35        100+          0-250      3\n",
      "4   Female    18-35        100+        251-600      7\n",
      "5   Female    18-35        100+           601+      3\n",
      "6   Female    18-35       36-60          0-250     70\n",
      "7   Female    18-35       36-60        251-600     85\n",
      "8   Female    18-35       36-60           601+     15\n",
      "9   Female    18-35      61-100          0-250     21\n",
      "10  Female    18-35      61-100        251-600     29\n",
      "11  Female    18-35      61-100           601+      5\n",
      "12  Female    36-55        0-35          0-250     97\n",
      "13  Female    36-55        0-35        251-600     93\n",
      "14  Female    36-55        0-35           601+      9\n",
      "15  Female    36-55        100+          0-250     25\n",
      "16  Female    36-55        100+        251-600     56\n",
      "17  Female    36-55        100+           601+     20\n",
      "18  Female    36-55       36-60          0-250    120\n",
      "19  Female    36-55       36-60        251-600    126\n",
      "20  Female    36-55       36-60           601+     26\n",
      "21  Female    36-55      61-100          0-250     64\n",
      "22  Female    36-55      61-100        251-600    119\n",
      "23  Female    36-55      61-100           601+     38\n",
      "24  Female      56+        0-35          0-250    275\n",
      "25  Female      56+        0-35        251-600    113\n",
      "26  Female      56+        0-35           601+      9\n",
      "27  Female      56+        100+          0-250     27\n",
      "28  Female      56+        100+        251-600     91\n",
      "29  Female      56+        100+           601+     46\n",
      "30  Female      56+       36-60          0-250     78\n",
      "31  Female      56+       36-60        251-600     78\n",
      "32  Female      56+       36-60           601+     14\n",
      "33  Female      56+      61-100          0-250     47\n",
      "34  Female      56+      61-100        251-600     89\n",
      "35  Female      56+      61-100           601+     22\n",
      "36    Male    18-35        0-35          0-250    255\n",
      "37    Male    18-35        0-35        251-600    220\n",
      "38    Male    18-35        0-35           601+     31\n",
      "39    Male    18-35        100+          0-250      4\n",
      "40    Male    18-35        100+        251-600      9\n",
      "41    Male    18-35        100+           601+      5\n",
      "42    Male    18-35       36-60          0-250     57\n",
      "43    Male    18-35       36-60        251-600     68\n",
      "44    Male    18-35       36-60           601+     21\n",
      "45    Male    18-35      61-100          0-250      9\n",
      "46    Male    18-35      61-100        251-600     26\n",
      "47    Male    18-35      61-100           601+     11\n",
      "48    Male    36-55        0-35          0-250    102\n",
      "49    Male    36-55        0-35        251-600     99\n",
      "50    Male    36-55        0-35           601+      8\n",
      "51    Male    36-55        100+          0-250     26\n",
      "52    Male    36-55        100+        251-600     69\n",
      "53    Male    36-55        100+           601+     29\n",
      "54    Male    36-55       36-60          0-250    107\n",
      "55    Male    36-55       36-60        251-600    129\n",
      "56    Male    36-55       36-60           601+     36\n",
      "57    Male    36-55      61-100          0-250     54\n",
      "58    Male    36-55      61-100        251-600    106\n",
      "59    Male    36-55      61-100           601+     45\n",
      "60    Male      56+        0-35          0-250    237\n",
      "61    Male      56+        0-35        251-600    161\n",
      "62    Male      56+        0-35           601+     11\n",
      "63    Male      56+        100+          0-250     33\n",
      "64    Male      56+        100+        251-600     76\n",
      "65    Male      56+        100+           601+     45\n",
      "66    Male      56+       36-60          0-250     70\n",
      "67    Male      56+       36-60        251-600     86\n",
      "68    Male      56+       36-60           601+     21\n",
      "69    Male      56+      61-100          0-250     36\n",
      "70    Male      56+      61-100        251-600     79\n",
      "71    Male      56+      61-100           601+     32\n"
     ]
    }
   ],
   "source": [
    "# === Creation of Equivalence Classes ===\n",
    "\n",
    "# Establish age ranges\n",
    "age_bins = [18, 35, 55, float('inf')]\n",
    "age_labels = ['18-35', '36-55', '56+']\n",
    "\n",
    "# Create an age group column\n",
    "data_of_interest['AgeGroup'] = pd.cut(data_of_interest['Age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Define the income ranges\n",
    "income_bins = [0, 36, 61, 101, float('inf')]\n",
    "income_labels = ['0-35', '36-60', '61-100', '100+']\n",
    "\n",
    "# Create a column for income group\n",
    "data_of_interest['IncomeGroup'] = pd.cut(data_of_interest['HouseholdIncome'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Define the card monthly spending groups\n",
    "card_spend_bins = [0, 251, 601, float('inf')]\n",
    "card_spend_labels = ['0-250','251-600','601+']  \n",
    "\n",
    "# Create a column for the card monthly spending group\n",
    "data_of_interest['CardSpendGroup'] = pd.cut(data_of_interest['CardSpendMonthly'], bins=card_spend_bins, labels=card_spend_labels)\n",
    "\n",
    "# Group by all variables to create equivalence classes\n",
    "equivalence_classes = data_of_interest.groupby(['Gender', 'AgeGroup', 'IncomeGroup', 'CardSpendGroup'])\n",
    "\n",
    "grouped_count = equivalence_classes.size()\n",
    "\n",
    "unique_equivalence_classes = equivalence_classes.groups\n",
    "\n",
    "unique_equivalence_classes = list(equivalence_classes.groups.keys())\n",
    "\n",
    "# Calculate the count of records in each equivalence class and create a count column\n",
    "grouped_count = equivalence_classes.size().reset_index(name='count')\n",
    "\n",
    "# Create a dataset of the equivalence classes\n",
    "equivalence_classes_table = pd.DataFrame(unique_equivalence_classes, columns=['Gender', 'AgeGroup', 'IncomeGroup', 'CardSpendGroup'])\n",
    "\n",
    "# Merge the equivalence classes dataset with the counts\n",
    "equivalence_classes_with_count = equivalence_classes_table.merge(grouped_count, on=['Gender', 'AgeGroup', 'IncomeGroup', 'CardSpendGroup'])\n",
    "\n",
    "# Print table with all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(equivalence_classes_with_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1ca206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender AgeGroup IncomeGroup CardSpendGroup  count\n",
      "3  Female    18-35        100+          0-250      3\n",
      "5  Female    18-35        100+           601+      3\n"
     ]
    }
   ],
   "source": [
    "# Check which ECs have the lowest counts\n",
    "\n",
    "equivalence_classes_filtered = equivalence_classes_with_count[equivalence_classes_with_count['count'] < 4]\n",
    "\n",
    "print(equivalence_classes_filtered)\n",
    "\n",
    "# The lowest count in an EC is 3, which implies an acceptable threshold risk of 33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba88fbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Risk %: 2.1950000000000003\n",
      "Maximum Risk %: 33.33\n"
     ]
    }
   ],
   "source": [
    "# === Assessment of Maximum and Median Risk ===\n",
    "\n",
    "# Maximum risk is 33% due to smallest EC has 3 records\n",
    "\n",
    "# Calculate reciprocal (1/count) and multiply by 100 of the count column to create a new column called risk%\n",
    "equivalence_classes_with_count['risk%'] = (1 / equivalence_classes_with_count['count']) * 100\n",
    "\n",
    "# Round risk% to two decimal places\n",
    "equivalence_classes_with_count['risk%'] = equivalence_classes_with_count['risk%'].round(2)\n",
    "\n",
    "# Median of the risk% column\n",
    "median_risk = equivalence_classes_with_count['risk%'].median()\n",
    "\n",
    "# Print median risk\n",
    "print(\"Median Risk %:\", median_risk)\n",
    "print(\"Maximum Risk %:\",33.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e801c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr(re-id) Median Risk (1/100): 0.00021950000000000002\n",
      "Pr(re-id) Maximum Risk (1/100): 0.0033000000000000004\n",
      "Pr(re-id) Median Risk (5/100): 0.0010975000000000002\n",
      "Pr(re-id) Maximum Risk (5/100): 0.0165\n",
      "Pr(re-id) Median Risk (10/100): 0.0021950000000000003\n",
      "Pr(re-id) Maximum Risk (10/100): 0.033\n"
     ]
    }
   ],
   "source": [
    "# === Scenario 1 ===\n",
    "\n",
    "# In this scenario we consider the situation of rogue employees that are handling the data. We will assume that 100 people will be handling this data set\n",
    "\n",
    "# In this example we will test a high risk (1/100) rogue employees, medium risk (5/10) rogue employees, and a conservative model (10/100) rogue employees\n",
    "\n",
    "\n",
    "# Median risk (1/100) \n",
    "\n",
    "median_risk_1 = (((0.02195) * (0.01))/0.01) * (0.01)\n",
    "\n",
    "print(\"Pr(re-id) Median Risk (1/100):\",median_risk_1)\n",
    "\n",
    "# Maximum risk (1/100)\n",
    "\n",
    "maximum_risk_1 = (((0.33) * (0.01))/0.01) * (0.01)\n",
    "\n",
    "print(\"Pr(re-id) Maximum Risk (1/100):\",maximum_risk_1)\n",
    "\n",
    "\n",
    "# Median risk (5/100)\n",
    "\n",
    "median_risk_2 = (((0.02195) * (0.05))/0.05) * (0.05)\n",
    "\n",
    "print(\"Pr(re-id) Median Risk (5/100):\",median_risk_2)\n",
    "\n",
    "# Maximum risk (5/100)\n",
    "\n",
    "maximum_risk_2 = (((0.33) * (0.05))/0.05) * (0.05)\n",
    "\n",
    "print(\"Pr(re-id) Maximum Risk (5/100):\",maximum_risk_2)\n",
    "\n",
    "\n",
    "# Median risk (10/100)\n",
    "\n",
    "median_risk_3 = (((0.02195) * (0.1))/0.1) * (0.1)\n",
    "\n",
    "print(\"Pr(re-id) Median Risk (10/100):\",median_risk_3)\n",
    "\n",
    "# Maximum risk (10/100)\n",
    "\n",
    "maximum_risk_3 = (((0.33) * (0.1))/0.1) * (0.1)\n",
    "\n",
    "print(\"Pr(re-id) Maximum Risk (10/100):\",maximum_risk_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "661c1eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr(re-id) Median Risk: 0.02195\n",
      "Pr(re-id) Maximum Risk: 0.33\n"
     ]
    }
   ],
   "source": [
    "# === Scenario 2 ===\n",
    "\n",
    "# In this scenario we consider an internal adversary where for example a data analyst working with the data set recognizes a record belonging to someone in their circle of acquaintances\n",
    "\n",
    "# It is estimated that 97% of Americans have a cellphone plan. While this data is being sold to a credit card company, this dataset contains individuals with a phone plan, so that is the relevant prevalence to be used here\n",
    "\n",
    "pr_acq = 1-(1-0.97)**150\n",
    "\n",
    "# Median risk\n",
    "\n",
    "pr_reid_median_risk = ((0.02195 * pr_acq) / pr_acq) * pr_acq\n",
    "\n",
    "print(\"Pr(re-id) Median Risk:\",pr_reid_median_risk)\n",
    "\n",
    "# Maximum risk\n",
    "\n",
    "pr_reid_maximum_risk = ((0.33 * pr_acq) / pr_acq) * pr_acq\n",
    "\n",
    "print(\"Pr(re-id) Maximum Risk:\",pr_reid_maximum_risk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf8b135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr(re-id) Median Risk: 0.0059265\n",
      "Pr(re-id) Maximum Risk: 0.08910000000000001\n"
     ]
    }
   ],
   "source": [
    "# === Scenario 3 ===\n",
    "\n",
    "# This scenario describes a situation in which a data breach occurs. Data is exposed outside of the intended recipients\n",
    "\n",
    "# Median risk\n",
    "\n",
    "pr_reid_median_risk_s3 = ((0.02195 * 0.27) / 0.27) * 0.27\n",
    "\n",
    "# Maximum risk\n",
    "\n",
    "pr_reid_maximum_risk_s3 = ((0.33 * 0.27) / 0.27) * 0.27\n",
    "\n",
    "print(\"Pr(re-id) Median Risk:\",pr_reid_median_risk_s3)\n",
    "\n",
    "print(\"Pr(re-id) Maximum Risk:\",pr_reid_maximum_risk_s3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4cf69b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr(re-id) Median Risk: 0.02195\n",
      "Pr(re-id) Maximum Risk: 0.33\n"
     ]
    }
   ],
   "source": [
    "# === Scenario 4 ===\n",
    "\n",
    "# This scenario considers the most catastrophic situation, in which the dataset becomes exposed to the public\n",
    "\n",
    "# Median risk\n",
    "\n",
    "pr_reid_median_risk_s4 = ((0.02195 * 1) / 1) * 1\n",
    "\n",
    "# Maximum risk\n",
    "\n",
    "pr_reid_maximum_risk_s4 = ((0.33 * 1) / 1) * 1\n",
    "\n",
    "print(\"Pr(re-id) Median Risk:\",pr_reid_median_risk_s4)\n",
    "\n",
    "print(\"Pr(re-id) Maximum Risk:\",pr_reid_maximum_risk_s4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a5f0a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          s1      s2      s3      s4\n",
      "risk                                \n",
      "<5%   84.72%  79.17%  93.06%  79.17%\n",
      "<10%  11.11%   5.56%   6.94%   5.56%\n",
      "<20%   4.17%  11.11%   0.00%  11.11%\n",
      "<33%   0.00%   1.39%   0.00%   1.39%\n",
      "<50%   0.00%   2.78%   0.00%   2.78%\n",
      ">50%   0.00%   0.00%   0.00%   0.00%\n"
     ]
    }
   ],
   "source": [
    "# === Results - Diagnostics Table ===\n",
    "\n",
    "# Create column with probability of reidentification\n",
    "\n",
    "equivalence_classes_with_count[\"pr_reid\"] = equivalence_classes_with_count[\"risk%\"] / 100\n",
    "\n",
    "# Iterate scenario 1 equation through each EC and create a new column with the respective values\n",
    "\n",
    "equivalence_classes_with_count[\"pr_reid_s1\"] = ((equivalence_classes_with_count[\"pr_reid\"] * 0.5) / 0.5) * 0.5\n",
    "\n",
    "# Classify each value in the pr_reid_s1 into their respect percentage groups\n",
    "\n",
    "equivalence_classes_with_count[\"risk%_group_s1\"] = pd.cut(\n",
    "    equivalence_classes_with_count[\"pr_reid_s1\"],\n",
    "    bins=[0, 0.05, 0.1, 0.2, 0.33, 0.5, float('inf')],\n",
    "    labels=[\"<5%\", \"<10%\", \"<20%\", \"<33%\", \"<50%\", \">50%\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Do the same with scenarion 2\n",
    "\n",
    "equivalence_classes_with_count[\"pr_reid_s2\"] = ((equivalence_classes_with_count[\"pr_reid\"] * 1) / 1) * 1\n",
    "\n",
    "equivalence_classes_with_count[\"risk%_group_s2\"] = pd.cut(\n",
    "    equivalence_classes_with_count[\"pr_reid_s2\"],\n",
    "    bins=[0, 0.05, 0.1, 0.2, 0.33, 0.5, float('inf')],\n",
    "    labels=[\"<5%\", \"<10%\", \"<20%\", \"<33%\", \"<50%\", \">50%\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Do the same with scenarion 3\n",
    "\n",
    "equivalence_classes_with_count[\"pr_reid_s3\"] = ((equivalence_classes_with_count[\"pr_reid\"] * 0.27) / 0.27) * 0.27\n",
    "\n",
    "equivalence_classes_with_count[\"risk%_group_s3\"] = pd.cut(\n",
    "    equivalence_classes_with_count[\"pr_reid_s3\"],\n",
    "    bins=[0, 0.05, 0.1, 0.2, 0.33, 0.5, float('inf')],\n",
    "    labels=[\"<5%\", \"<10%\", \"<20%\", \"<33%\", \"<50%\", \">50%\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Do the same with scenarion 4\n",
    "\n",
    "equivalence_classes_with_count[\"pr_reid_s4\"] = ((equivalence_classes_with_count[\"pr_reid\"] * 1) / 1) * 1\n",
    "\n",
    "equivalence_classes_with_count[\"risk%_group_s4\"] = pd.cut(\n",
    "    equivalence_classes_with_count[\"pr_reid_s4\"],\n",
    "    bins=[0, 0.05, 0.1, 0.2, 0.33, 0.5, float('inf')],\n",
    "    labels=[\"<5%\", \"<10%\", \"<20%\", \"<33%\", \"<50%\", \">50%\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Create table\n",
    "\n",
    "# List risk groups\n",
    "risk_groups = [\"<5%\", \"<10%\", \"<20%\", \"<33%\", \"<50%\", \">50%\"]\n",
    "\n",
    "# Create dictionary to store data for each s value\n",
    "summary_data = {\"risk\": risk_groups}\n",
    "\n",
    "# Iterate through each s value\n",
    "for s in range(1, 5):\n",
    "    percentages = []\n",
    "    for risk_group in risk_groups:\n",
    "        column_name = f\"risk%_group_s{s}\"\n",
    "        count = (equivalence_classes_with_count[column_name] == risk_group).sum()\n",
    "        total = len(equivalence_classes_with_count)\n",
    "        percentage = (count / total) * 100\n",
    "# Format values with 2 decimal points and a % sign\n",
    "        percentages.append(f\"{percentage:.2f}%\")\n",
    "        \n",
    "    summary_data[f\"s{s}\"] = percentages\n",
    "\n",
    "# Create the summary table from the dictionary\n",
    "summary_table = pd.DataFrame(summary_data)\n",
    "summary_table.set_index(\"risk\", inplace=True)\n",
    "\n",
    "# Print table\n",
    "print(summary_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "375b6adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a4be7_row2_col0, #T_a4be7_row2_col2 {\n",
       "  background-color: green;\n",
       "}\n",
       "#T_a4be7_row2_col1, #T_a4be7_row2_col3 {\n",
       "  background-color: red;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a4be7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a4be7_level0_col0\" class=\"col_heading level0 col0\" >s1</th>\n",
       "      <th id=\"T_a4be7_level0_col1\" class=\"col_heading level0 col1\" >s2</th>\n",
       "      <th id=\"T_a4be7_level0_col2\" class=\"col_heading level0 col2\" >s3</th>\n",
       "      <th id=\"T_a4be7_level0_col3\" class=\"col_heading level0 col3\" >s4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a4be7_level0_row0\" class=\"row_heading level0 row0\" >median risk</th>\n",
       "      <td id=\"T_a4be7_row0_col0\" class=\"data row0 col0\" >0.11%</td>\n",
       "      <td id=\"T_a4be7_row0_col1\" class=\"data row0 col1\" >2.2%</td>\n",
       "      <td id=\"T_a4be7_row0_col2\" class=\"data row0 col2\" >0.59%</td>\n",
       "      <td id=\"T_a4be7_row0_col3\" class=\"data row0 col3\" >2.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4be7_level0_row1\" class=\"row_heading level0 row1\" >maximum risk</th>\n",
       "      <td id=\"T_a4be7_row1_col0\" class=\"data row1 col0\" >1.65%</td>\n",
       "      <td id=\"T_a4be7_row1_col1\" class=\"data row1 col1\" >33.33%</td>\n",
       "      <td id=\"T_a4be7_row1_col2\" class=\"data row1 col2\" >8.91%</td>\n",
       "      <td id=\"T_a4be7_row1_col3\" class=\"data row1 col3\" >33.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a4be7_level0_row2\" class=\"row_heading level0 row2\" >assessment</th>\n",
       "      <td id=\"T_a4be7_row2_col0\" class=\"data row2 col0\" >pass</td>\n",
       "      <td id=\"T_a4be7_row2_col1\" class=\"data row2 col1\" >fail</td>\n",
       "      <td id=\"T_a4be7_row2_col2\" class=\"data row2 col2\" >pass</td>\n",
       "      <td id=\"T_a4be7_row2_col3\" class=\"data row2 col3\" >fail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1347f8790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Results - Conclusion Table ===\n",
    "\n",
    "#  Create a dictionary with data\n",
    "data = {\n",
    "    's1': ['0.11%', '1.65%', 'pass'],\n",
    "    's2': ['2.2%', '33.33%', 'fail'],\n",
    "    's3': ['0.59%', '8.91%', 'pass'],\n",
    "    's4': ['2.2%', '33.33%', 'fail']\n",
    "}\n",
    "\n",
    "# Create a df\n",
    "df = pd.DataFrame(data, index=['median risk', 'maximum risk', 'assessment'])\n",
    "\n",
    "# apply color styling\n",
    "def highlight_cells(val):\n",
    "    if val == 'pass':\n",
    "        return 'background-color: green'\n",
    "    elif val == 'fail':\n",
    "        return 'background-color: red'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply styling to the df\n",
    "color_table = df.style.applymap(highlight_cells)\n",
    "\n",
    "# Display table\n",
    "color_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452962c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
